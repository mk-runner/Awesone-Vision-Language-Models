# Awesone-Vision-Language-Models

## Table of Contents

- [Survey](#survey)
- [Dataset](#Dataset)
- [Metrics](#Metrics)
- [Papers](#papers)
  - [2024](#2024)

## Survey
- *** [[paper](https://arxiv.org/abs/2311.14199)]

## Papers
### 2025
**arXiv'25**
- SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features [[paper](https://arxiv.org/pdf/2502.14786)][[code](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md)]


### 2024
**NeurIPS'24**
- ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models [[paper](https://arxiv.org/abs/2407.21534)][[code](https://github.com/mrwu-mac/ControlMLLM)]
- Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning [[paper](https://openreview.net/pdf?id=0bINeW40u4)]

**arXiv'24**
- POINTS: IMPROVING YOUR VISION-LANGUAGE MODEL WITH AFFORDABLE STRATEGIES [[paper](https://arxiv.org/pdf/2409.04828)][[code](https://github.com/WePOINTS/WePOINTS)]
- CONTRASTIVE LOCALIZED LANGUAGE-IMAGE PRE-TRAINING [[paper](https://arxiv.org/pdf/2410.02746)]
- ARIA : An Open Multimodal Native Mixture-of-Experts Model [[paper](https://arxiv.org/pdf/2410.05993)][[code](https://github.com/rhymes-ai/Aria)]
