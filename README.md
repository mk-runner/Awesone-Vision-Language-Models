# Awesone-Vision-Language-Models

## Table of Contents

- [Survey](#survey)
- [Dataset](#Dataset)
- [Metrics](#Metrics)
- [Papers](#papers)
  - [2024](#2024)

## Survey
- ***

## Papers
### 2025
**CVPR'25**
- Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation [[paper](https://arxiv.org/pdf/2504.18856v1)][[code](https://github.com/BasitAlawode/MR-PLIP)]


**NAACL'25**
- VividMed: Vision Language Model with Versatile Visual Grounding for Medicine [[paper](https://arxiv.org/pdf/2410.12694)][[code](https://github.com/function2-llx/MMMM)]

**arXiv'25**
- SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features [[paper](https://arxiv.org/pdf/2502.14786)][[code](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md)]
- MedTrim Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models [[paper](https://arxiv.org/pdf/2504.15929)][[code](https://github.com/icon-lab/MedTrim)]


### 2024
**NeurIPS'24**
- ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models [[paper](https://arxiv.org/abs/2407.21534)][[code](https://github.com/mrwu-mac/ControlMLLM)]
- Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning [[paper](https://openreview.net/pdf?id=0bINeW40u4)]

**arXiv'24**
- POINTS: IMPROVING YOUR VISION-LANGUAGE MODEL WITH AFFORDABLE STRATEGIES [[paper](https://arxiv.org/pdf/2409.04828)][[code](https://github.com/WePOINTS/WePOINTS)]
- CONTRASTIVE LOCALIZED LANGUAGE-IMAGE PRE-TRAINING [[paper](https://arxiv.org/pdf/2410.02746)]
- ARIA : An Open Multimodal Native Mixture-of-Experts Model [[paper](https://arxiv.org/pdf/2410.05993)][[code](https://github.com/rhymes-ai/Aria)]
