# Awesone-Vision-Language-Models

## Table of Contents

- [Survey](#survey)
- [Dataset](#Dataset)
- [Metrics](#Metrics)
- [Papers](#papers)
  - [2025](#2025)
  - [2024](#2024)

## Survey
- Multimodal Large Language Models for Medicine: A Comprehensive Survey [[paper](https://arxiv.org/pdf/2504.21051)]

## Papers
### 2025
**CVPR'25**
- Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation [[paper](https://arxiv.org/pdf/2504.18856v1)][[code](https://github.com/BasitAlawode/MR-PLIP)]
- Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zou_Alignment_Mining_and_Fusion_Representation_Alignment_with_Hard_Negative_Mining_CVPR_2025_paper.pdf)]

**MedIA'25**
- ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training [[paper](https://arxiv.org/pdf/2312.13316)]

**TMM'25**
- Multi-Grained Vision-and-Language Model for Medical Image and Text Alignment [[paper](https://ieeexplore.ieee.org/abstract/document/11091540)]

**NAACL'25**
- VividMed: Vision Language Model with Versatile Visual Grounding for Medicine [[paper](https://arxiv.org/pdf/2410.12694)][[code](https://github.com/function2-llx/MMMM)]

**arXiv'25**
- SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features [[paper](https://arxiv.org/pdf/2502.14786)][[code](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md)]
- MedTrim Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models [[paper](https://arxiv.org/pdf/2504.15929)][[code](https://github.com/icon-lab/MedTrim)]
- UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation [[paper](https://arxiv.org/pdf/2504.21336)]


### 2024
**NeurIPS'24**
- ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models [[paper](https://arxiv.org/abs/2407.21534)][[code](https://github.com/mrwu-mac/ControlMLLM)]
- Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning [[paper](https://openreview.net/pdf?id=0bINeW40u4)]

**arXiv'24**
- POINTS: IMPROVING YOUR VISION-LANGUAGE MODEL WITH AFFORDABLE STRATEGIES [[paper](https://arxiv.org/pdf/2409.04828)][[code](https://github.com/WePOINTS/WePOINTS)]
- CONTRASTIVE LOCALIZED LANGUAGE-IMAGE PRE-TRAINING [[paper](https://arxiv.org/pdf/2410.02746)]
- ARIA : An Open Multimodal Native Mixture-of-Experts Model [[paper](https://arxiv.org/pdf/2410.05993)][[code](https://github.com/rhymes-ai/Aria)]
